{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from azureml.core import Workspace\n",
        "import os\n",
        "import io\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1723292586719
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe_to_blob(dataframe, container_name, blob_name):\n",
        "    # Get connection string from environment variables\n",
        "    connection_string = os.getenv('connection_string')\n",
        "    if not connection_string:\n",
        "        raise ValueError(\"connection_string is not set in the .env file\")\n",
        "    # Initialize BlobServiceClient\n",
        "    try:\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "        print(\"Successfully connected to Azure Blob Storage.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Error initializing BlobServiceClient: {e}\")\n",
        "        raise\n",
        "    # Ensure the container exists\n",
        "    try:\n",
        "        container_client = blob_service_client.get_container_client(container_name)\n",
        "        if not container_client.exists():\n",
        "            container_client.create_container()\n",
        "            print(f\"Created container: {container_name}\")\n",
        "        else:\n",
        "            print(f\"Container {container_name} already exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/getting container client: {e}\")\n",
        "        raise\n",
        "    # Convert dataframe to CSV string\n",
        "    csv_data = dataframe.to_csv(index=False)\n",
        "    # Upload CSV string to blob storage\n",
        "    try:\n",
        "        blob_client = container_client.get_blob_client(blob_name)\n",
        "        blob_client.upload_blob(csv_data, overwrite=True)\n",
        "        print(f\"Uploaded {blob_name} to blob storage in container {container_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading blob: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_dataframe_from_blob(container_name, blob_name):\n",
        "    \"\"\"\n",
        "    Loads a CSV file from Azure Blob Storage into a Pandas DataFrame.\n",
        "    Args:\n",
        "        container_name (str): The name of the Azure Blob Storage container.\n",
        "        blob_name (str): The name of the blob to download.\n",
        "    Returns:\n",
        "        pandas.DataFrame: The loaded DataFrame.\n",
        "    \"\"\"\n",
        "    # Get connection string from environment variables\n",
        "    connection_string = os.getenv('connection_string')\n",
        "    if not connection_string:\n",
        "        raise ValueError(\"connection_string is not set in the .env file\")\n",
        "    # Initialize BlobServiceClient\n",
        "    try:\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "        print(\"Successfully connected to Azure Blob Storage.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Error initializing BlobServiceClient: {e}\")\n",
        "        raise\n",
        "    # Get blob client\n",
        "    try:\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting blob client: {e}\")\n",
        "        raise\n",
        "    # Download blob content to a byte stream\n",
        "    download_stream = blob_client.download_blob()\n",
        "    blob_data = download_stream.readall()\n",
        "    # Create a Pandas DataFrame from the byte stream\n",
        "    df = pd.read_csv(io.BytesIO(blob_data))\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292397682
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "connection_string = os.getenv('connection_string')\n",
        "container_name = os.getenv('container_name')\n",
        "blob_name = os.getenv('train_blob_name')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292402611
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_processed = load_dataframe_from_blob('processed-files','processed_train_df.csv')\n",
        "print(df_train_processed.shape)\n",
        "print('')\n",
        "df_train_processed.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Successfully connected to Azure Blob Storage.\n(455401, 11)\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "                                UserID  basket_icon_click  basket_add_list  \\\n0  a720-6b732349-a720-4862-bd21-644732                  0                0   \n1  a0c0-6b73247c-a0c0-4bd9-8baa-797356                  0                0   \n2  86a8-6b735c67-86a8-407b-ba24-333055                  0                0   \n3  6a3d-6b736346-6a3d-4085-934b-396834                  0                0   \n4  b74a-6b737717-b74a-45c3-8c6a-421140                  0                1   \n\n   basket_add_detail  image_picker  list_size_dropdown  \\\n0                  0             0                   0   \n1                  0             0                   0   \n2                  0             0                   0   \n3                  0             0                   0   \n4                  0             0                   1   \n\n   closed_minibasket_click  sign_in  saw_checkout  saw_homepage  ordered  \n0                        0        0             0             0        0  \n1                        0        0             0             0        0  \n2                        0        0             0             0        0  \n3                        0        0             0             0        0  \n4                        0        1             1             1        1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserID</th>\n      <th>basket_icon_click</th>\n      <th>basket_add_list</th>\n      <th>basket_add_detail</th>\n      <th>image_picker</th>\n      <th>list_size_dropdown</th>\n      <th>closed_minibasket_click</th>\n      <th>sign_in</th>\n      <th>saw_checkout</th>\n      <th>saw_homepage</th>\n      <th>ordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a720-6b732349-a720-4862-bd21-644732</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a0c0-6b73247c-a0c0-4bd9-8baa-797356</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>86a8-6b735c67-86a8-407b-ba24-333055</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6a3d-6b736346-6a3d-4085-934b-396834</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b74a-6b737717-b74a-45c3-8c6a-421140</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292405698
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_processed[\"ordered\"].value_counts"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "<bound method IndexOpsMixin.value_counts of 0         0\n1         0\n2         0\n3         0\n4         1\n         ..\n455396    0\n455397    0\n455398    0\n455399    0\n455400    0\nName: ordered, Length: 455401, dtype: int64>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723021706531
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_train_processed.drop(columns=['UserID','ordered'])\n",
        "y = df_train_processed['ordered']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292413667
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution in training and validation sets\n",
        "def check_class_distribution(y_train, y_val):\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "    unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
        "    print(\"Training set class distribution:\", dict(zip(unique_train, counts_train)))\n",
        "    print(\"Validation set class distribution:\", dict(zip(unique_val, counts_val)))\n",
        "\n",
        "check_class_distribution(y_train, y_val)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training set class distribution: {0: 349046, 1: 15274}\nValidation set class distribution: {0: 87262, 1: 3819}\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292418500
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to apply models\n",
        "def apply_model(model, X_train, y_train, X_val, y_val, drop_id_col_list):\n",
        "    # Fit the model\n",
        "    model.fit(X_train.drop(drop_id_col_list, axis=1, errors='ignore'), y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train.drop(drop_id_col_list, axis=1, errors='ignore'))\n",
        "    y_pred = model.predict(X_val.drop(drop_id_col_list, axis=1, errors='ignore'))\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
        "    accuracy_val = accuracy_score(y_val, y_pred)\n",
        "    f1_train = f1_score(y_train,y_train_pred)\n",
        "    f1_val = f1_score(y_val, y_pred)\n",
        "    return accuracy_train, accuracy_val,f1_train,f1_val\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292504074
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the models\n",
        "vanila_models = [\n",
        "    (\"Logistic Regression\", LogisticRegression(random_state=321)),\n",
        "    (\"Decision Tree\", DecisionTreeClassifier(random_state=321)),\n",
        "    (\"Random Forest\", RandomForestClassifier(random_state=321)),\n",
        "    (\"XGB Classifier\", XGBClassifier(random_state=321))\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723292594964
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#id col not to be considered while training\n",
        "drop_id_col_list = ['UserID','ordered']\n",
        "\n",
        "# Applying the models and storing the results\n",
        "results_model_name = []\n",
        "results_accuracy_val = []\n",
        "results_f1_score_val = []\n",
        "results_accuracy_train = []\n",
        "results_f1_score_train = []\n",
        "\n",
        "for name, model in vanila_models:\n",
        "    accuracy_train, accuracy_val,f1_train,f1_val = apply_model(model, X_train, y_train, X_val, y_val,drop_id_col_list)\n",
        "    results_model_name.append(name)\n",
        "    results_accuracy_train.append(accuracy_train)\n",
        "    results_accuracy_val.append(accuracy_val)\n",
        "    results_f1_score_train.append(f1_train)\n",
        "    results_f1_score_val.append(f1_val)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[12:32:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723293150960
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(columns=['Model_Name',\n",
        "                                   'Accuracy_Train','Accuracy_Val',\n",
        "                                   'F1_Score_Train','F1_Score_Val'])\n",
        "\n",
        "results_df['Model_Name'] = results_model_name\n",
        "results_df['Accuracy_Train'] = results_accuracy_train\n",
        "results_df['Accuracy_Val'] = results_accuracy_val\n",
        "results_df['F1_Score_Train'] = results_f1_score_train\n",
        "results_df['F1_Score_Val'] = results_f1_score_val\n",
        "\n",
        "print(results_df.to_string())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "            Model_Name  Accuracy_Train  Accuracy_Val  F1_Score_Train  F1_Score_Val\n0  Logistic Regression        0.973210      0.973167        0.681960      0.679853\n1        Decision Tree        0.975220      0.975253        0.738576      0.736805\n2        Random Forest        0.975220      0.975297        0.738698      0.737395\n3       XGB Classifier        0.975217      0.975330        0.738963      0.738325\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723293156842
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# models = {\n",
        "#     \"XGBoost\": XGBClassifier(objective='binary:logistic', random_state=42),\n",
        "#     \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "#     \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42)\n",
        "# }\n",
        "\n",
        "# # Evaluate each model and log results with MLflow\n",
        "# for model_name, model in models.items():\n",
        "#     with mlflow.start_run(run_name=model_name):\n",
        "#         # Train the model\n",
        "#         model.fit(X_train, y_train)\n",
        "        \n",
        "#         # Make predictions\n",
        "#         y_train_pred = model.predict(X_train)\n",
        "#         y_val_pred = model.predict(X_val)\n",
        "        \n",
        "#         # Training metrics\n",
        "#         train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "#         train_precision = precision_score(y_train, y_train_pred)\n",
        "#         train_recall = recall_score(y_train, y_train_pred)\n",
        "#         train_f1 = f1_score(y_train, y_train_pred)\n",
        "\n",
        "#         # Validation metrics\n",
        "#         val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "#         val_precision = precision_score(y_val, y_val_pred)\n",
        "#         val_recall = recall_score(y_val, y_val_pred)\n",
        "#         val_f1 = f1_score(y_val, y_val_pred)\n",
        "#         report = classification_report(y_val, y_val_pred)\n",
        "        \n",
        "#         # Log model, parameters, and metrics\n",
        "#         mlflow.sklearn.log_model(model, \"model\")\n",
        "#         mlflow.log_params(model.get_params())\n",
        "        \n",
        "#         # Log training metrics\n",
        "#         mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
        "#         mlflow.log_metric(\"train_precision\", train_precision)\n",
        "#         mlflow.log_metric(\"train_recall\", train_recall)\n",
        "#         mlflow.log_metric(\"train_f1_score\", train_f1)\n",
        "        \n",
        "#         # Log validation metrics\n",
        "#         mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
        "#         mlflow.log_metric(\"val_precision\", val_precision)\n",
        "#         mlflow.log_metric(\"val_recall\", val_recall)\n",
        "#         mlflow.log_metric(\"val_f1_score\", val_f1)\n",
        "        \n",
        "#         # Log the classification report as an artifact\n",
        "#         report_path = \"classification_report.txt\"\n",
        "#         with open(report_path, \"w\") as f:\n",
        "#             f.write(report)\n",
        "#         mlflow.log_artifact(report_path)\n",
        "    \n",
        "#         # Print the evaluation report\n",
        "#         print(f\"Model: {model_name}\")\n",
        "#         print(f\"Train Accuracy: {train_accuracy}\")\n",
        "#         print(f\"Train Precision: {train_precision}\")\n",
        "#         print(f\"Train Recall: {train_recall}\")\n",
        "#         print(f\"Train F1 Score: {train_f1}\")\n",
        "#         print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "#         print(f\"Validation Precision: {val_precision}\")\n",
        "#         print(f\"Validation Recall: {val_recall}\")\n",
        "#         print(f\"Validation F1 Score: {val_f1}\")\n",
        "#         print(report)\n",
        "#         print(\"=\"*80)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723293210784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def objective(params):\n",
        "#     with mlflow.start_run(nested=True):\n",
        "#         model = XGBClassifier(**params, objective='binary:logistic', random_state=42, use_label_encoder=False)\n",
        "#         model.fit(X_train, y_train)\n",
        "        \n",
        "#         # Predictions\n",
        "#         y_train_pred = model.predict(X_train)\n",
        "#         y_val_pred = model.predict(X_val)\n",
        "\n",
        "#         # Training metrics\n",
        "#         train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "#         train_precision = precision_score(y_train, y_train_pred)\n",
        "#         train_recall = recall_score(y_train, y_train_pred)\n",
        "#         train_f1 = f1_score(y_train, y_train_pred)\n",
        "\n",
        "#         # Validation metrics\n",
        "#         val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "#         val_precision = precision_score(y_val, y_val_pred)\n",
        "#         val_recall = recall_score(y_val, y_val_pred)\n",
        "#         val_f1 = f1_score(y_val, y_val_pred)\n",
        "#         report = classification_report(y_val, y_val_pred)\n",
        "        \n",
        "#         # Log model, parameters, and metrics\n",
        "#         mlflow.sklearn.log_model(model, \"model\")\n",
        "#         mlflow.log_params(params)\n",
        "        \n",
        "#         # Log training metrics\n",
        "#         mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
        "#         mlflow.log_metric(\"train_precision\", train_precision)\n",
        "#         mlflow.log_metric(\"train_recall\", train_recall)\n",
        "#         mlflow.log_metric(\"train_f1_score\", train_f1)\n",
        "\n",
        "#         # Log validation metrics\n",
        "#         mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
        "#         mlflow.log_metric(\"val_precision\", val_precision)\n",
        "#         mlflow.log_metric(\"val_recall\", val_recall)\n",
        "#         mlflow.log_metric(\"val_f1_score\", val_f1)\n",
        "        \n",
        "#         # Log the classification report as an artifact\n",
        "#         report_path = \"classification_report.txt\"\n",
        "#         with open(report_path, \"w\") as f:\n",
        "#             f.write(report)\n",
        "#         mlflow.log_artifact(report_path)\n",
        "\n",
        "#         return {'loss': -val_f1, 'status': STATUS_OK, 'val_accuracy': val_accuracy, 'val_precision': val_precision, 'val_recall': val_recall, 'val_f1': val_f1}\n",
        "\n",
        "# # Define the search space\n",
        "# space = {\n",
        "#     'n_estimators': hp.choice('n_estimators', range(50, 500)),\n",
        "#     'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "#     'max_depth': hp.choice('max_depth', range(3, 15)),\n",
        "#     'min_child_weight': hp.choice('min_child_weight', range(1, 10)),\n",
        "#     'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "#     'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
        "# }\n",
        "\n",
        "# # Run the optimization\n",
        "# trials = Trials()\n",
        "# best = fmin(fn=objective,\n",
        "#             space=space,\n",
        "#             algo=tpe.suggest,\n",
        "#             max_evals=50,\n",
        "#             trials=trials)\n",
        "\n",
        "# # Convert hyperopt results to real parameter values\n",
        "# best_params = {\n",
        "#     'n_estimators': best['n_estimators'],\n",
        "#     'learning_rate': best['learning_rate'],\n",
        "#     'max_depth': best['max_depth'] + 3,  # adding the minimum value of range\n",
        "#     'min_child_weight': best['min_child_weight'] + 1,\n",
        "#     'subsample': best['subsample'],\n",
        "#     'colsample_bytree': best['colsample_bytree']\n",
        "# }\n",
        "\n",
        "# print(\"Best parameters found: \", best_params)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723293228349
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_train, y_train, X_val, y_val, drop_id_col_list):\n",
        "   model.fit(X_train.drop(drop_id_col_list, axis=1, errors='ignore'), y_train)\n",
        "   y_pred_train = model.predict(X_train.drop(drop_id_col_list, axis=1, errors='ignore'))\n",
        "   y_pred_val = model.predict(X_val.drop(drop_id_col_list, axis=1, errors='ignore'))\n",
        "\n",
        "   f1_train = f1_score(y_train, y_pred_train)\n",
        "   accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "   f1_val = f1_score(y_val, y_pred_val)\n",
        "   accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "\n",
        "   print(\"Training Set:\")\n",
        "   print(\"F1-score:\", f1_train)\n",
        "   print(\"Accuracy:\", accuracy_train)\n",
        "   print(\"\\nValidation Set:\")\n",
        "   print(\"F1-score:\", f1_val)\n",
        "   print(\"Accuracy:\", accuracy_val)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723294294313
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Tracking"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment name\n",
        "experiment_name = \"Azure_Propensity_Model\"\n",
        "mlflow.set_experiment(experiment_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024/08/10 12:42:04 INFO mlflow.tracking.fluent: Experiment with name 'Azure_Propensity_Model' does not exist. Creating a new experiment.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "<Experiment: artifact_location='', creation_time=1723293725726, experiment_id='e50b013a-522b-4df2-b552-1787132ac6e8', last_update_time=None, lifecycle_stage='active', name='Azure_Propensity_Model', tags={}>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723293722984
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1: Logistic Regression**\n",
        "- Set Search Space\n",
        "- Write Objective function\n",
        "- Train LR and fetch best Params\n",
        "- fit the final model\n",
        "- Evaluate Results\n",
        "- Log Model to MLflow"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter spaces for each classifier\n",
        "space_lr = {\n",
        "    'C': hp.loguniform('C', np.log(0.01), np.log(10)),\n",
        "    'max_iter': hp.uniform('max_iter', 1,5000)\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723295048797
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_evaluate(params):\n",
        "    # Define columns to drop from the dataset\n",
        "    drop_id_col_list = ['UserID','ordered']\n",
        "\n",
        "    # Create a LogisticRegression model with the given parameters\n",
        "    model = LogisticRegression(**params, random_state=321)\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train.drop(drop_id_col_list, axis=1, errors='ignore'), y_train)\n",
        "    \n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_val.drop(drop_id_col_list, axis=1, errors='ignore'))\n",
        "    score = f1_score(y_val, y_pred)\n",
        "    \n",
        "    return -score  # Minimize the negative of F1-score\n"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723295051294
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trials for hyperparameter optimization\n",
        "trials = Trials()\n",
        "\n",
        "# Use Hyperopt to search for the best hyperparameters\n",
        "best_LR_param = fmin(fn=train_evaluate, space=space_lr, algo=tpe.suggest, max_evals=10, trials=trials)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_LR_param)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "100%|██████████| 10/10 [00:11<00:00,  1.13s/trial, best loss: -0.6798532879224521]\nBest hyperparameters: {'C': 2.3330541258369792, 'max_iter': 1470.762397734738}\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723295064894
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr = LogisticRegression(**best_LR_param)\n",
        "model_lr.fit(X_train.drop(drop_id_col_list, axis=1, errors='ignore'), y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "LogisticRegression(C=2.3330541258369792, max_iter=1470.762397734738)",
            "text/html": "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=2.3330541258369792, max_iter=1470.762397734738)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=2.3330541258369792, max_iter=1470.762397734738)</pre></div></div></div></div></div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723295076330
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model_lr,X_train,y_train,X_val,y_val,drop_id_col_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training Set:\nF1-score: 0.68198110133594\nAccuracy: 0.9732103645147123\n\nValidation Set:\nF1-score: 0.6798532879224521\nAccuracy: 0.9731667416914614\n"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723295093537
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Start an MLflow run\n",
        "mlflow.set_experiment('Azure_Propensity_Model')\n",
        "\n",
        "with mlflow.start_run(run_name=\"LogisticReg_classifier_azure_mlops\") as run:\n",
        "    # Log the best hyperparameters\n",
        "    mlflow.log_params(best_LR_param)\n",
        "    \n",
        "    # Train the final model with the best hyperparameters\n",
        "    best_model = LogisticRegression(**best_LR_param, random_state=321)\n",
        "    best_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model on the test dataset\n",
        "    y_pred = best_model.predict(X_val)\n",
        "    \n",
        "    # Log model performance metrics\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    y_pred_prob = best_model.predict_proba(X_val)[:, 1]\n",
        "    auc_roc = roc_auc_score(y_val, y_pred_prob)\n",
        "\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"auc_roc\", auc_roc)\n",
        "    \n",
        "    # Log the shapes of training and testing data\n",
        "    mlflow.log_param(\"train_shape\", X_train.shape)\n",
        "    mlflow.log_param(\"test_shape\", X_val.shape)\n",
        "    \n",
        "    # Plot and log the ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC-ROC (area = {auc_roc:.2f})\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Save the plot as a temporary file\n",
        "    plt_path = \"roc_curve.png\"\n",
        "    plt.savefig(plt_path)\n",
        "    \n",
        "    # Log the ROC curve\n",
        "    mlflow.log_artifact(plt_path)\n",
        "    \n",
        "    # Log the trained model\n",
        "    mlflow.sklearn.log_model(best_model, \"LogisticReg_classifier_azure_mlops\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "print(\"Model and metrics logged in MLflow successfully.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model and metrics logged in MLflow successfully.\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723295615531
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}